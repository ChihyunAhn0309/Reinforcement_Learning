{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChihyunAhn0309/Reinforcement_Learning/blob/main/25S_CS377_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS377 Reinforcement Learning - Programming Assignment 2\n"
      ],
      "metadata": {
        "id": "kIZTN6FJusJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Guideline"
      ],
      "metadata": {
        "id": "2_ISUOkHq0v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to submit**\n",
        "*   Fill out üõë <mark> TODO</mark> blocks, **DO NOT** modify other parts of the skeleton code.\n",
        "*   Submit one file: hw2_{student_ID}.ipynb to KLMS\n",
        "\n",
        "    e.g. hw2_20251234.ipynb\n",
        "* **Before submission, do not forget to click Runtime -> Restart and run all**\n",
        "* **Late submission policy**: After the submission deadline, you will immediately lose 10% of the score, another 10% after 24 hours later, and so on. Submissions after 72 hours (3 days) will not be counted.\n",
        "\n",
        "**Note**\n",
        "*   You are required to use numpy, do not use neither pytorch nor tensorflow.\n",
        "*   Do not change the random SEED\n",
        "*   Check whether your whole cells work well by restarting runtime code and running all before the submission.\n",
        "*   TA will look into the implemented functions, their validity and give corresponding score to each TODO problem.\n",
        "*   It is recommended to use Google Colab by uploading the notebook first, then for the submission, you need to download the notebook with `.ipynb`. However, it is still possible to run the notebook on your local device as well.\n",
        "*    TA in charge: Doojin Baek (doojin.a.baek@kaist.ac.kr)\n",
        "\n",
        "\n",
        "Please feel free to ask **on KLMS** any questions! Good luck üòÉ"
      ],
      "metadata": {
        "id": "_vdZaH2Rq3TK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sources**:\n",
        "\n",
        "- \"Reinforcement Learning, An Introduction\" book by Richard Sutton, chapters 5,6, and 7.\n",
        "- [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/posts/2018-02-19-rl-overview/) by Lilian Weng\n",
        "\n",
        "**In this programming assignment, you will**\n",
        "* Implement Monte-Carlo prediction (first-visit)\n",
        "* Implement Monte-Carlo control with epsilon-greedy policy\n",
        "* Implement TD prediction (TD(0))\n",
        "* Implement two TD control methods: Q-Learning, and SARSA\n",
        "* Learn about three new environment in Gym package: [FrozenLake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), [BlackJack](https://www.gymlibrary.dev/environments/toy_text/blackjack/), and [CliffWalking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/)\n",
        "\n",
        "**Before submitting this programming assignment, please be sure that you did all the üõë<mark>TODOs</mark>**"
      ],
      "metadata": {
        "id": "XNF325fh79WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Essentials\n",
        "\n",
        "In this section, we are doing the following:\n",
        "- Installing the required packages\n",
        "- Importing the required packages\n",
        "- Define helper utilities to be used later in the assignment"
      ],
      "metadata": {
        "id": "vjap3CqzfafZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNrn_Sn8mji9"
      },
      "outputs": [],
      "source": [
        "# install required packages and tools for in-notebook visualization\n",
        "# Just a workaround solution by replacing the ubuntu repository to some Kakao mirros\n",
        "!sed -i 's|http://archive.ubuntu|http://mirror.kakao|g' /etc/apt/sources.list\n",
        "!sed -i 's|http://security.ubuntu|http://mirror.kakao|g' /etc/apt/sources.list\n",
        "!apt update &&apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!apt install chromium-browser xvfb\n",
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gymnasium\n",
        "!pip install tqdm seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required packages\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from functools import partial # https://docs.python.org/3/library/functools.html#functools.partial\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# matplotlib.style.use('ggplot')\n",
        "\n",
        "SEED = 1234"
      ],
      "metadata": {
        "id": "YTLKK8S6feKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities for display and rendering\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "3EraFL3clfk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper utilities"
      ],
      "metadata": {
        "id": "kZIqR1lLro83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "def fix_seed(env, seed):\n",
        "    \"\"\"\n",
        "    It sets the seed for the random number generator in Python, NumPy, and OpenAI Gym\n",
        "\n",
        "    :param env: the environment\n",
        "    :param seed: the random seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    env.reset(seed=seed)"
      ],
      "metadata": {
        "id": "QKisQVrpKI7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "def run_env_w_policy(env, policy=None, steps=1000, sleep=0.1, render=True):\n",
        "    \"\"\"\n",
        "    > `run_env_w_policy` runs an environment for a given number of steps, and renders the environment at\n",
        "    each step\n",
        "\n",
        "    :param env: the environment to run\n",
        "    :param policy: a function that takes in an observation and returns an action (optional)\n",
        "    :param steps: the number of steps to run the environment for, defaults to 1000 (optional)\n",
        "    :param sleep: how long to wait between frames (optional)\n",
        "    :param render: if True, render the environment, defaults to True (optional)\n",
        "    \"\"\"\n",
        "    def render_func():\n",
        "        if render:\n",
        "            screen = env.render()\n",
        "            plt.imshow(screen)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "            time.sleep(sleep)\n",
        "\n",
        "    if render and env.render_mode is None:\n",
        "        raise ValueError(\"Use rendered_env instead of env\")\n",
        "\n",
        "    if policy is None:\n",
        "        policy = lambda _: env.action_space.sample()\n",
        "\n",
        "    observation, _ = env.reset(seed=SEED)  # start new episode\n",
        "    render_func()\n",
        "    for i in range(steps):\n",
        "        # this is for visualization\n",
        "        action = policy(observation)  # sample random action from env\n",
        "        observation, reward, terminated, truncated, info = env.step(action)  # next state\n",
        "        render_func()\n",
        "        if terminated:  # if the episode is over\n",
        "            if i == steps:\n",
        "                print(\"Reached maximum number of steps\")\n",
        "            break\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "6v4cEqcD_r2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "def run_env_random_policy(env, steps=10, sleep=0.1):\n",
        "    \"\"\"\n",
        "    It runs an environment for a given number of steps with a random policy, and sleeps for a given amount of time between\n",
        "    each step\n",
        "\n",
        "    :param env: the environment to run\n",
        "    :param steps: the number of steps to run the environment for, defaults to 10 (optional)\n",
        "    :param sleep: how long to wait between each step (optional)\n",
        "    \"\"\"\n",
        "    run_env_w_policy(env, None, steps, sleep)"
      ],
      "metadata": {
        "id": "KDZdRv1F-Whl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "def rollout(policy, env, num_steps):\n",
        "    \"\"\"\n",
        "    It runs the policy for a given number of steps, and returns the episode\n",
        "\n",
        "    :param policy: The policy to be used for the rollout\n",
        "    :param env: The environment to run the policy in\n",
        "    :param num_steps: The number of steps to run the rollout for\n",
        "    :return: A list of lists. Each list contains the state, action, reward, and next state.\n",
        "    \"\"\"\n",
        "    state, _ = env.reset()\n",
        "    episode = []\n",
        "    for step in range(num_steps):\n",
        "        action = policy(state)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode.append([state, action, reward, next_state])\n",
        "        if terminated:\n",
        "            break\n",
        "        state = next_state\n",
        "    return episode"
      ],
      "metadata": {
        "id": "jAKjaZQYEXKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "def compute_episode_return(episode, gamma=1):\n",
        "    \"\"\"\n",
        "    It takes a list of steps in an episode and returns the sum of the discounted rewards\n",
        "\n",
        "    :param episode: a list of (state, action, reward, and next state) tuples\n",
        "    :param gamma: discount factor, defaults to 1 (optional)\n",
        "    :return: The sum of the rewards for each step in the episode, multiplied by the discount factor.\n",
        "    \"\"\"\n",
        "    return sum([step[2]*(gamma**i) for i, step in enumerate(episode)])\n",
        "\n",
        "# <DO NOT CHANGE>\n",
        "def compute_episode_length(episode):\n",
        "    \"\"\"\n",
        "    It takes in an episode and returns the length of the episode\n",
        "\n",
        "    :param episode: a list of (state, action, reward) tuples\n",
        "    :return: The length of the episode.\n",
        "    \"\"\"\n",
        "    return len(episode)"
      ],
      "metadata": {
        "id": "tvXb14pLFPu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Class\n",
        "\n",
        "No need to fully understand this part of the code. This class has been written to help solving the assignment by wrapping everything related to the environment from running the environment in renderless mode to plotting the value function."
      ],
      "metadata": {
        "id": "1xQjvSD1jB_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "# > The `Env` class is considered as a wrapper. It contains the environment itself, a\n",
        "# rendered version of the environment, and a few helper functions.\n",
        "class Env:\n",
        "    def __init__(self, name, env, rendered_env, restricted_policy=None, plot_Vfunction=None, plot_Vfunction_from_Q=None, plot_dim=None):\n",
        "        \"\"\"\n",
        "        The function takes in the name of the agent, the environment, the rendered environment, the\n",
        "        restricted policy, the plot V function, and the plot Q function.\n",
        "\n",
        "        :param name: The name of the agent\n",
        "        :param env: the environment without rendering\n",
        "        :param rendered_env: This is the environment that rendering is enabled\n",
        "        :param restricted_policy: a function that represent a restricted policy for the environment\n",
        "        :param plot_Vfunction: a function that takes in Vfunction and plots it\n",
        "        :param plot_Vfunction_from_Q: a function that takes in a Qfunction and plots from it Vfunction\n",
        "        :param plot_dim: dimensions for the plotting of the value function table\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.env = env\n",
        "        self.rendered_env = rendered_env\n",
        "        self._restricted_policy = restricted_policy\n",
        "        self._plot_Vfunction = plot_Vfunction\n",
        "        self._plot_Vfunction_from_Q = plot_Vfunction_from_Q\n",
        "        self.plot_dim = plot_dim\n",
        "\n",
        "    @property\n",
        "    def restricted_policy(self):\n",
        "        if self._restricted_policy is None:\n",
        "            # return a policy that selects only from the half of the observation space\n",
        "            return lambda _: np.random.randint(self.env.action_space.n//2)\n",
        "        return self._restricted_policy\n",
        "\n",
        "    @property\n",
        "    def plot_Vfunction(self):\n",
        "        if self._plot_Vfunction is None:\n",
        "            def _plot_Vfunction(V):\n",
        "                fig = plt.figure(figsize=(10,5))\n",
        "                s_dim = self.plot_dim #(discrete_obs_space[:2])\n",
        "                value_mat = np.zeros(s_dim)\n",
        "                min_val, max_val = min(V.values()), max(V.values())\n",
        "                for k,v in V.items():\n",
        "                    if min_val == max_val:\n",
        "                        value_mat[k//s_dim[1], k%s_dim[1]] = v\n",
        "                        continue\n",
        "                    value_mat[k//s_dim[1], k%s_dim[1]] = (v - min_val) / (max_val - min_val)\n",
        "                sns.heatmap(value_mat, cmap=\"flare\", center=0.00, annot=True,  linewidths=.5)\n",
        "                plt.show()\n",
        "            return _plot_Vfunction\n",
        "            # raise NotImplementedError(f\"Env {self.name} has no plot_Vfunction\")\n",
        "        return self._plot_Vfunction\n",
        "\n",
        "    @property\n",
        "    def plot_Vfunction_from_Q(self):\n",
        "        if self._plot_Vfunction_from_Q is None:\n",
        "            def to_Vfunction(Q):\n",
        "                V = defaultdict(float)\n",
        "                for state, actions in Q.items():\n",
        "                    action_value = np.max(actions)\n",
        "                    V[state] = action_value\n",
        "                return V\n",
        "            return lambda Q: self.plot_Vfunction(to_Vfunction(Q))\n",
        "        return self._plot_Vfunction\n",
        "\n",
        "    def plot_episode_stats(self, stats, smoothing_window=10, noshow=False):\n",
        "        # Plot the episode length over time\n",
        "        fig1 = plt.figure(figsize=(10,5))\n",
        "        plt.plot(stats[\"len\"])\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Episode Length\")\n",
        "        plt.title(\"Episode Length over Time\")\n",
        "        if noshow:\n",
        "            plt.close(fig1)\n",
        "        else:\n",
        "            plt.show(fig1)\n",
        "\n",
        "        # Plot the episode reward over time\n",
        "        fig2 = plt.figure(figsize=(10,5))\n",
        "        rewards_smoothed = pd.Series(stats[\"rew\"]).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "        plt.plot(rewards_smoothed)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Episode Reward (Smoothed)\")\n",
        "        plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
        "        if noshow:\n",
        "            plt.close(fig2)\n",
        "        else:\n",
        "            plt.show(fig2)\n",
        "\n",
        "        return fig1, fig2\n",
        "\n",
        "    def get_random_policy(self):\n",
        "        return lambda _: self.env.action_space.sample()"
      ],
      "metadata": {
        "id": "QhnP5HdDKhrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hints: please refer to Env class definition for more understanding\n",
        "\n",
        "# To access the environment without rendering, use:\n",
        "# env_obj.env\n",
        "\n",
        "# To access the environment with rendering, use:\n",
        "# env_obj.rendered_env\n",
        "\n",
        "# To access the restricted_policy, use:\n",
        "# env_obj.restricted_policy\n",
        "\n",
        "# Take a step in renderless environment:\n",
        "# env_obj.env.step(action)\n",
        "\n",
        "# Take a step in rendered environment:\n",
        "# env_obj.rendered_env.step(action)"
      ],
      "metadata": {
        "id": "wtGDM_0jOsYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environments\n",
        "\n",
        "During the previous programming assignment, you have been introudced to OpenAI Gym. In this programming assignment, we will use FrozenLake and other two environments."
      ],
      "metadata": {
        "id": "8DmmIxFwfel-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. FrozenLake environment\n",
        "\n",
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "- Read and understand about FrozenLake environment's action space, observation space, and rewards from: [here](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) (No need to report or write anything to solve this todo)"
      ],
      "metadata": {
        "id": "pagLc-tb-oSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![alt text](https://gymnasium.farama.org/_images/frozen_lake.gif)"
      ],
      "metadata": {
        "id": "hw0xq-RI_C_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
        "rendered_env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array').unwrapped\n",
        "frozenlake = Env(\"FrozenLake\", env, rendered_env, plot_dim=(4,4))"
      ],
      "metadata": {
        "id": "IhHLiSkNj7b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recap about Gym environments"
      ],
      "metadata": {
        "id": "EoZktP7Syph0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for using the environment:\n",
        "env = frozenlake.env\n",
        "# You need to reset the environment before taking a step\n",
        "observation, _ = env.reset()\n",
        "terminated = False\n",
        "while not terminated:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    print(observation, reward, terminated, truncated, info)"
      ],
      "metadata": {
        "id": "T8XfJ8Nhyf6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_env_random_policy(frozenlake.rendered_env, steps=5)"
      ],
      "metadata": {
        "id": "eL0PBVya_d-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Visualization of the environment\n",
        "\n",
        "No need to run these cells"
      ],
      "metadata": {
        "id": "PZEPwCb-vE6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the restricted policy\n",
        "run_env_w_policy(frozenlake.rendered_env, frozenlake.restricted_policy, steps=5)"
      ],
      "metadata": {
        "id": "C6H5__pkpO5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the random policy\n",
        "run_env_w_policy(frozenlake.rendered_env, frozenlake.get_random_policy(), steps=5)"
      ],
      "metadata": {
        "id": "Uoqfw7mRqfW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Blackjack environment\n",
        "\n",
        "Blackjack is a card game where the goal is to beat the dealer by obtaining cards that sum to closer to 21 (without going over 21) than the dealers cards. Blackjack has observation space larger than frozenlake, furthermore, in blackjack, we do not have the access to the transition probabilities.\n",
        "\n",
        "Therefore, previous methods from the previuos assignment are not applicable here in the case of unknown dynamics.\n",
        "\n",
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "- Read and understand about Blackjack environment's action space, observation space, and rewards from: [here](https://www.gymlibrary.dev/environments/toy_text/blackjack/) (No need to report or write anything to solve this todo)\n",
        "\n",
        "![alt text](https://www.gymlibrary.dev/_images/blackjack.gif)"
      ],
      "metadata": {
        "id": "-8tPfb7N_F1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "rendered_env = gym.make(\"Blackjack-v1\", render_mode='rgb_array')\n",
        "def restricted_policy(observation):\n",
        "    \"\"\"\n",
        "    If the player's score is 20 or greater, stop asking for cards (i.e., return action 0), otherwise,\n",
        "    ask for another card (i.e., return action 1)\n",
        "\n",
        "    :param observation: the current state of the game\n",
        "    :return: The policy is being returned.\n",
        "    \"\"\"\n",
        "    # print(observation)\n",
        "    score, dealer_score, usable_ace = observation\n",
        "    return 0 if score >= 20 else 1\n",
        "\n",
        "\n",
        "# Using 3D surface plotting\n",
        "def plot_surface_Vfunction(V, title = \"Value Function\"):\n",
        "    '''\n",
        "    Plots the value function as a surface plot.\n",
        "    '''\n",
        "    min_x = 11 #min(k[0] for k in V.keys())\n",
        "    max_x = max(k[0] for k in V.keys())\n",
        "    min_y = min(k[1] for k in V.keys())\n",
        "    max_y = max(k[1] for k in V.keys())\n",
        "\n",
        "    x_range = np.arange(min_x, max_x + 1)\n",
        "    y_range = np.arange(min_y, max_y + 1)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "    # Find value for all (x, y) coordinates\n",
        "    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n",
        "    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n",
        "\n",
        "    def plot_surface(X, Y, Z, title):\n",
        "        fig = plt.figure(figsize = (20, 10))\n",
        "        ax = fig.add_subplot(111, projection = '3d')\n",
        "        surf = ax.plot_surface(X, Y, Z, rstride = 1, cstride = 1,\n",
        "                               cmap = matplotlib.cm.coolwarm, vmin = -1.0, vmax = 1.0)\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_zlabel('Value')\n",
        "        ax.set_title(title)\n",
        "        ax.view_init(ax.elev, -120)\n",
        "        fig.colorbar(surf)\n",
        "        plt.show()\n",
        "\n",
        "    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n",
        "    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n",
        "\n",
        "# Using heatmap\n",
        "def plot_Vfunction(V):\n",
        "    s_dim = (32,11)\n",
        "    value_mat_false = np.zeros(s_dim)\n",
        "    value_mat_true = np.zeros(s_dim)\n",
        "    for k,v in V.items():\n",
        "        if k[2]:\n",
        "            value_mat_true[k[0], k[1]] = v\n",
        "        else:\n",
        "            value_mat_false[k[0], k[1]] = v\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(20, 8))\n",
        "\n",
        "    sns.heatmap(value_mat_false, cmap=\"flare\", center=0.00, annot=True,  linewidths=.5, ax=ax1)\n",
        "    ax1.set_title('No Usable Ace')\n",
        "\n",
        "    sns.heatmap(value_mat_true, cmap=\"flare\", center=0.00, annot=True,  linewidths=.5, ax=ax2)\n",
        "    ax2.set_title('Usable Ace')\n",
        "    fig.tight_layout()\n",
        "\n",
        "blackjack = Env(\"BlackJack\", env, rendered_env, restricted_policy, plot_Vfunction)"
      ],
      "metadata": {
        "id": "bQwmo8GdfhMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "Print action and observation spaces of Blackjack and a sample from the action space"
      ],
      "metadata": {
        "id": "RyUY2ImCzosq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO ########\n",
        "print() # action space\n",
        "print() # observation space\n",
        "print() # action space's sample\n",
        "######################"
      ],
      "metadata": {
        "id": "pOBTgQ-ozoNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_env_random_policy(blackjack.rendered_env, steps=5)"
      ],
      "metadata": {
        "id": "WFymPd22fhZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Visualization of the environment\n",
        "\n",
        "No need to run these cells"
      ],
      "metadata": {
        "id": "SPY_Nycvu4ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the restricted policy\n",
        "run_env_w_policy(blackjack.rendered_env, blackjack.restricted_policy, steps=5)"
      ],
      "metadata": {
        "id": "biENrVkBqhuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the random policy\n",
        "run_env_w_policy(blackjack.rendered_env, blackjack.get_random_policy(), steps=5)"
      ],
      "metadata": {
        "id": "XCIZWW9Gnv3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Cliffwalking environment\n",
        "It is very similar to frozenlake but with larger observation space\n",
        "\n",
        "Adapted from Example 6.6 (page 106) from [Reinforcement Learning: An Introduction by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n",
        "\n",
        "üõë <mark> TODO:</mark>\n",
        "\n",
        "- Read and understand about CliffWalking environment's action space, observation space, and rewards from: [here](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) (No need to report or write anything to solve this todo)\n",
        "\n",
        "![alt text](https://www.gymlibrary.dev/_images/cliff_walking.gif)"
      ],
      "metadata": {
        "id": "Qi2QErQOJBu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "env = gym.make(\"CliffWalking-v0\")\n",
        "rendered_env = gym.make(\"CliffWalking-v0\", render_mode='rgb_array')\n",
        "cliffwalking = Env(\"CliffWalking\", env, rendered_env, plot_dim=(4,12))"
      ],
      "metadata": {
        "id": "kip4ojp2JAuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 Visualization of the environment\n",
        "\n",
        "No need to run these cells"
      ],
      "metadata": {
        "id": "1M61a16AEPOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_env_random_policy(cliffwalking.rendered_env, steps=5)"
      ],
      "metadata": {
        "id": "GHGUQdHeuywN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the restricted policy\n",
        "run_env_w_policy(cliffwalking.rendered_env, cliffwalking.restricted_policy, steps=5)"
      ],
      "metadata": {
        "id": "v3xtllXcuywO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the random policy\n",
        "run_env_w_policy(cliffwalking.rendered_env, cliffwalking.get_random_policy(), steps=5)"
      ],
      "metadata": {
        "id": "jMMTIDq1uywO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Monte-Carlo\n",
        "\n",
        "Monte-Carlo (MC) learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return. As, in the case of MDP with unknown dynamics in which Dynamic Programming methods are not able to be used.\n",
        "\n",
        "To compute the empirical return $G_t$, MC methods need to learn from **complete** episodes to compute $$G_t=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$$ and all the episodes must eventually terminate."
      ],
      "metadata": {
        "id": "FIsOpOidgvdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 MC Prediction\n",
        "\n",
        "Here, we are intrested to evaluate a policy by computing the value functions.\n",
        "\n",
        "In this assignment, we are interested to predict the state-value function $V(s)$, and it can be estimated through the following equation:\n",
        "\n",
        "$$V(s) = \\frac{\\sum_{t=1}^{T}\\mathbb{1}[S_t = s] G_t}{\\sum_{t=1}^{T}\\mathbb{1}[S_t = s]}$$"
      ],
      "metadata": {
        "id": "hRsfxKjDpjrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 First-visit predicition\n",
        "\n",
        "There are two ways for MC predicition:\n",
        "\n",
        "1. Every-visit predicition: We may count the visit of state s every time so that there could exist multiple visits of one state in one episode\n",
        "2. First-visit predicition: only count it the first time we encounter a state in one episode\n",
        "\n",
        "\n",
        "In this assignment, we are only interested in First-visit method\n"
      ],
      "metadata": {
        "id": "U40dWSw_tDjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "In the following section, you are required to implement Monte-Carlo First-Visit Prediction"
      ],
      "metadata": {
        "id": "A6S7ADt02_n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: RL Sutton book, 5.1\n",
        "def mc_prediciton_first_visit(policy, env_obj, num_episodes, episode_length=100, discount_factor=1.0, seed=SEED):\n",
        "    \"\"\"\n",
        "    > For each episode, we find the first visit of each state, and then calculate the return of the\n",
        "    state based on the first visit\n",
        "\n",
        "    :param policy: The policy to be evaluated\n",
        "    :param env_obj: The environment object that include a gym environment to run the algorithm on\n",
        "    :param num_episodes: The number of episodes to run\n",
        "    :param episode_length: The length of each episode, defaults to 100 (optional)\n",
        "    :param discount_factor: The discount factor for the MDP\n",
        "    :param seed: The random seed to use for the environment\n",
        "    :return: The value function as a dictionary with keys with size and values\n",
        "    \"\"\"\n",
        "    env = env_obj.env\n",
        "    fix_seed(env, seed)\n",
        "\n",
        "    # Storing the value fucntion table\n",
        "    V = defaultdict(float)\n",
        "    # Hint: You can access the elements in V like this: V[state] = scalar\n",
        "    #       in which the state can be a tuple\n",
        "\n",
        "    ######## TODO ########\n",
        "\n",
        "    ######################\n",
        "    return V"
      ],
      "metadata": {
        "id": "-scJqi4Pgzkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1.1 Blackjack environment"
      ],
      "metadata": {
        "id": "smFBJpjh1Yct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = mc_prediciton_first_visit(blackjack.restricted_policy, blackjack, num_episodes=10000)\n",
        "# blackjack.plot_Vfunction(V)\n",
        "plot_surface_Vfunction(V)"
      ],
      "metadata": {
        "id": "mb-10DwWJUMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = mc_prediciton_first_visit(blackjack.restricted_policy, blackjack.env, num_episodes=50000)\n",
        "# blackjack.plot_Vfunction(V)\n",
        "plot_surface_Vfunction(V)"
      ],
      "metadata": {
        "id": "IcXpKtTriEFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = mc_prediciton_first_visit(blackjack.get_random_policy(), blackjack, num_episodes=10000)\n",
        "# blackjack.plot_Vfunction(V)\n",
        "plot_surface_Vfunction(V)"
      ],
      "metadata": {
        "id": "357BTS0GlKUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = mc_prediciton_first_visit(blackjack.get_random_policy(), blackjack, num_episodes=50000)\n",
        "# blackjack.plot_Vfunction(V)\n",
        "plot_surface_Vfunction(V)"
      ],
      "metadata": {
        "id": "fbq0WABxjsPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1.2 Frozenlake environment"
      ],
      "metadata": {
        "id": "lg4dvVWz1eAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = mc_prediciton_first_visit(frozenlake.get_random_policy(), frozenlake, num_episodes=500)\n",
        "frozenlake.plot_Vfunction(V)"
      ],
      "metadata": {
        "id": "vPLje7Ofm5Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = mc_prediciton_first_visit(frozenlake.restricted_policy, frozenlake, num_episodes=500)\n",
        "frozenlake.plot_Vfunction(V)"
      ],
      "metadata": {
        "id": "2YCJ665El2lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "Answer the following question in the following cell:\n",
        "\n",
        "Q: What is the reason that the predicted value function of the restricted policy is full of zeros while the random policy is not?\n"
      ],
      "metadata": {
        "id": "us1bfex-9BPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "üõë <mark>TODO: Answer</mark>\n",
        "\n",
        "(No more than 100 words)\n",
        "\n",
        "--------------\n",
        "\n"
      ],
      "metadata": {
        "id": "XifWV4dP-qsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 MC Control\n",
        "\n",
        "MC control is concerned to learn the optimal policy by simply evaluating the policy"
      ],
      "metadata": {
        "id": "vzfUbuyAq9C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Epsilon-Greedy"
      ],
      "metadata": {
        "id": "eMwFDRQJs8KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <DO NOT CHANGE>\n",
        "\n",
        "def epsilon_greedy_policy_fn(observation, Q, epsilon, env):\n",
        "    \"\"\"\n",
        "    It takes in an observation, a Q-table, an epsilon value, and an environment, and returns an action\n",
        "\n",
        "    :param observation: the current state of the environment\n",
        "    :param Q: The Q-table\n",
        "    :param epsilon: the probability of taking a random action\n",
        "    :param env: the environment\n",
        "    :return: The action that is being returned is the action that is being taken by the agent.\n",
        "    \"\"\"\n",
        "    action = None\n",
        "    if np.random.rand() < epsilon:\n",
        "        # random action\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        # best action\n",
        "        action = np.argmax(Q[observation])\n",
        "    return action"
      ],
      "metadata": {
        "id": "DigDeil6zwrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "In the following section, you are required to implement Monte-Carlo Control using the epsilon greedy policy based on first-visit"
      ],
      "metadata": {
        "id": "Em-egKvd4Bjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: https://web.stanford.edu/class/cs234/slides/lecture4post.pdf\n",
        "def mc_control_epsilon_greedy(env_obj, num_episodes, episode_length=100, discount_factor=1.0, epsilon=0.1, seed=SEED):\n",
        "    \"\"\"\n",
        "    For each episode, we generate a rollout using the epsilon-greedy policy, and then we update the\n",
        "    action-value function with the return using first-visit Monte Carlo method.\n",
        "\n",
        "    :param env_obj: The environment object that include a gym environment to run the algorithm on\n",
        "    :param num_episodes: The number of episodes to run\n",
        "    :param episode_length: The length of each episode, defaults to 100 (optional)\n",
        "    :param discount_factor: The discount factor for the MDP\n",
        "    :param epsilon: The probability of selecting a random action. Float between 0 and 1\n",
        "    :param seed: The random seed to use for the environment\n",
        "    :return:\n",
        "        * Q-function: a dictionary, such that the key is (state, action) and the value is the action-state-value\n",
        "        * policy: an epsilon greedy policy used during the training based on the computed Q-table\n",
        "        * stats: a dictionary, such that it has two keys rew & len, and the values are the total reward and length of the episode, respectively.\n",
        "    \"\"\"\n",
        "    env = env_obj.env\n",
        "    fix_seed(env, seed)\n",
        "    # A nested dictionary that maps state -> (action -> action-value).\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # Hint: You can access the elements in Q like this: Q[state][action]\n",
        "\n",
        "    # Store the reward and length of episodes during the running of the method\n",
        "    stats = defaultdict(list)\n",
        "\n",
        "    ######## TODO ########\n",
        "    # Note (Important):\n",
        "\n",
        "    # For each episode, make sure that you call:\n",
        "    # stats[\"rew\"].append(compute_episode_return(episode))\n",
        "    # stats[\"len\"].append(compute_episode_length(episode))\n",
        "    # such that episode is a list such that each element is a list of [state, action, reward, next_state]\n",
        "    #   Example: [[s1,a1,r1,s2], [s2,a2,r2,s3], ....]\n",
        "\n",
        "    #   Hint: episode is the return of calling rollout()\n",
        "    #   Hint: you can use: policy = partial(epsilon_greedy_policy_fn, Q=Q, epsilon=epsilon, env=env)\n",
        "\n",
        "\n",
        "    ######################\n",
        "    return Q, policy, stats"
      ],
      "metadata": {
        "id": "he9NoWPYs7u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.1 Blackjack environment"
      ],
      "metadata": {
        "id": "e6da8F0n1zTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q, policy, stats = mc_control_epsilon_greedy(blackjack, num_episodes=50000, epsilon=0.1)\n",
        "# Create value function from action-value function by picking the best action at each state\n",
        "V = defaultdict(float)\n",
        "for state, actions in Q.items():\n",
        "    action_value = np.max(actions)\n",
        "    V[state] = action_value\n",
        "# blackjack.plot_Vfunction(V) # if you want to see it in heatmap\n",
        "plot_surface_Vfunction(V)"
      ],
      "metadata": {
        "id": "L-cuo1HMBy9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.2 Frozenlake environment"
      ],
      "metadata": {
        "id": "cCAIJc2M2AB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q, policy, stats = mc_control_epsilon_greedy(frozenlake, num_episodes=50000, epsilon=0.1)\n",
        "# Create value function from action-value function by picking the best action at each state\n",
        "V = defaultdict(float)\n",
        "for state, actions in Q.items():\n",
        "    action_value = np.max(actions)\n",
        "    V[state] = action_value\n",
        "frozenlake.plot_Vfunction(V)"
      ],
      "metadata": {
        "id": "--D4A0MhB1Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "Answer the following question in the following cell:\n",
        "\n",
        "Q: Does it give you a matrix full of zeros? If yes, explain why?\n",
        "\n",
        "Q: Try to change epsilon in the following code cell, then explain why that happened!\n",
        "\n",
        "Q: And is it an optimal policy? and why?\n"
      ],
      "metadata": {
        "id": "zNrnqMSyNTaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "--------------\n",
        "\n",
        "üõë <mark>TODO: Answer</mark>\n",
        "\n",
        "(No more than 100 words for each question)\n",
        "\n",
        "\n",
        "--------------\n"
      ],
      "metadata": {
        "id": "UK8BqvwONluZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO ########\n",
        "epsilon = ?\n",
        "Q, policy, stats = mc_control_epsilon_greedy(frozenlake, num_episodes=50000, epsilon=epsilon)\n",
        "# Create value function from action-value function by picking the best action at each state\n",
        "V = defaultdict(float)\n",
        "for state, actions in Q.items():\n",
        "    action_value = np.max(actions)\n",
        "    V[state] = action_value\n",
        "frozenlake.plot_Vfunction(V)\n",
        "######################"
      ],
      "metadata": {
        "id": "-LzCe2pzMy5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Temporal-Difference Learning\n",
        "\n",
        "Starting from here, we are going to experiment with cliffwalking environment.\n",
        "\n",
        "Similar to Monte-Carlo methods, Temporal-Difference (TD) Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we don‚Äôt need to track the episode up to termination. TD learning is so important that Sutton & Barto (2017) in their RL book describes it as ‚Äúone idea ‚Ä¶ central and novel to reinforcement learning‚Äù.\n"
      ],
      "metadata": {
        "id": "lZzM3pbxokwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 TD(0) Prediction\n",
        "\n",
        "For TD(0)'s target $=R_{t+1}+\\gamma V(S_{t+1})$. Therefore, the state-value function can be estimated: $$V(S_t) ‚Üê V(S_t) + \\alpha \\left[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\right]$$"
      ],
      "metadata": {
        "id": "4_Ugpz8QlaqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "Answer the following question in the following cell:\n",
        "\n",
        "Q: What is the state-value function update rule of Monte-Carlo every-visit predicition, Monte-Carlo First-visit predicition, TD(0), TD(1), and TD($\\lambda$)?"
      ],
      "metadata": {
        "id": "10qbbMjo3-ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "üõë <mark>TODO: Answer</mark>\n",
        "\n",
        "(No more than 100 words)\n",
        "\n",
        "--------------\n"
      ],
      "metadata": {
        "id": "J5bjTLuT4Mfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "In the following section, you are required to implement TD(0)"
      ],
      "metadata": {
        "id": "4gMCEGke4Oh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: RL Sutton book, 6.1\n",
        "def TD0(policy, env_obj, num_episodes, episode_length=100, discount_factor=1.0, seed=SEED, alpha=0.5, verbose=True):\n",
        "    \"\"\"\n",
        "    > For each episode, we run a rollout and update the value function for each state in the episode\n",
        "    following TD(0) method\n",
        "\n",
        "    :param policy: the policy to be evaluated\n",
        "    :param env_obj: The environment object that include a gym environment to run the algorithm on\n",
        "    :param num_episodes: number of episodes to run\n",
        "    :param episode_length: The number of steps in each episode, defaults to 100 (optional)\n",
        "    :param discount_factor: The discount factor for the MDP\n",
        "    :param seed: The random seed used to generate the episode\n",
        "    :param alpha: learning rate\n",
        "    :param verbose: If True, it will plot the value function every 10% of the episodes, defaults to True\n",
        "    (optional)\n",
        "    :return: The value function table\n",
        "    \"\"\"\n",
        "    env = env_obj.env\n",
        "    logging_freq = int(num_episodes*0.1)\n",
        "    fix_seed(env, seed)\n",
        "\n",
        "    # Storing the value fucntion table\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    ######## TODO ########\n",
        "    # Note (Important):\n",
        "\n",
        "    # For each episode, make sure that you call:\n",
        "    # * if verbose and (episode_idx+1)%logging_freq == 0:\n",
        "    #       env_obj.plot_Vfunction(V)\n",
        "\n",
        "\n",
        "    # Hint: V dictionary has the following form to access its values: V[state]\n",
        "\n",
        "\n",
        "    ######################\n",
        "    return V"
      ],
      "metadata": {
        "id": "exGys0dfldG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = TD0(cliffwalking.get_random_policy(), cliffwalking, num_episodes=1000)\n",
        "cliffwalking.plot_Vfunction(V)"
      ],
      "metadata": {
        "id": "TqNYS06PKJho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Q-Learning (Off-policy TD control)\n",
        "\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + Œ± * (r + Œ≥ * max(Q(s', a')) - Q(s, a))$$\n",
        "\n",
        "Where:\n",
        "- Q(s, a): the current Q-value for state 's' and action 'a'\n",
        "- Œ±: the learning rate (0 <= Œ± <= 1)\n",
        "- r: the immediate reward for taking action 'a' in state 's'\n",
        "- Œ≥: the discount factor (0 <= Œ≥ <= 1)\n",
        "- max(Q(s', a')): the maximum Q-value for the next state 's' and any possible action 'a'"
      ],
      "metadata": {
        "id": "UXlcq2jtg0Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "In the following section, you are required to implement Q-Learning"
      ],
      "metadata": {
        "id": "8Iz9UZbst5pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: RL Sutton book, 6.5\n",
        "\n",
        "def QLearning(env_obj, num_episodes, episode_length=100, discount_factor=1.0, epsilon=0.1, seed=SEED, alpha=0.5, verbose=True):\n",
        "    \"\"\"\n",
        "    > For each episode, we run a loop for a maximum of `episode_length` steps. In each step, we take an\n",
        "    action according to the epsilon-greedy policy, observe the reward and next state, and update the\n",
        "    Q-value of the state-action pair\n",
        "\n",
        "    :param env_obj: The environment object that include a gym environment to run the algorithm on\n",
        "    :param num_episodes: number of episodes to run\n",
        "    :param episode_length: The maximum length of an episode, defaults to 100 (optional)\n",
        "    :param discount_factor: The discount factor for the MDP\n",
        "    :param epsilon: the probability of choosing a random action\n",
        "    :param seed: The random seed to use for the environment\n",
        "    :param alpha: learning rate\n",
        "    :param verbose: If True, it will print the progress of the algorithm each specific frequency, defaults to True (optional)\n",
        "    :return:\n",
        "        * Q-function: a dictionary, such that the key is (state, action) and the value is the action-state-value\n",
        "        * policy: an epsilon greedy policy used during the training based on the computed Q-table\n",
        "        * stats: a dictionary, such that it has two keys rew & len, and the values are the total reward and length of the episode, respectively.\n",
        "    \"\"\"\n",
        "    env = env_obj.env\n",
        "    logging_freq = int(num_episodes*0.1)\n",
        "    fix_seed(env, seed)\n",
        "    # A nested dictionary that maps state -> (action -> action-value).\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # Store the reward and length of episodes during the running of the method\n",
        "    stats = defaultdict(list)\n",
        "\n",
        "    ######## TODO ########\n",
        "    # Note (Important):\n",
        "\n",
        "    # For each episode, make sure that you call:\n",
        "    # * stats[\"rew\"].append(cumulative_reward)\n",
        "    # * stats[\"len\"].append(episode_length)\n",
        "    # Such that cumulative_reward is the total reward in the episode\n",
        "    #  and episode_length stores the length of the episode\n",
        "    # * if verbose and (episode_idx+1)%logging_freq == 0:\n",
        "    #       env_obj.plot_Vfunction_from_Q(Q)\n",
        "\n",
        "    # Hint: you can use: policy = partial(epsilon_greedy_policy_fn, Q=Q, epsilon=epsilon, env=env)\n",
        "\n",
        "    # Hint: Q dictionary has the following form to access its values: Q[state][action]\n",
        "\n",
        "    ######################\n",
        "\n",
        "    return Q, policy, stats"
      ],
      "metadata": {
        "id": "tEfQQ9NmiGyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_obj = cliffwalking\n",
        "qlearning_Q, policy, stats = QLearning(env_obj, num_episodes=1000, episode_length=1000)\n",
        "env_obj.plot_Vfunction_from_Q(qlearning_Q)\n",
        "env_obj.plot_episode_stats(stats)"
      ],
      "metadata": {
        "id": "yJjG15oTiHNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the policy from the Q-function\n",
        "policy = lambda s: np.argmax(qlearning_Q[s])\n",
        "run_env_w_policy(cliffwalking.rendered_env, policy, steps=50)"
      ],
      "metadata": {
        "id": "fAzr2t2Jjt7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 SARSA (On-policy TD control)\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + Œ± * (r + Œ≥ * Q(s', a') - Q(s, a))$$\n",
        "\n",
        "Where:\n",
        "- Q(s, a): the current Q-value for state 's' and action 'a'\n",
        "- Œ±: the learning rate (0 <= Œ± <= 1)\n",
        "- r: the immediate reward for taking action 'a' in state 's'\n",
        "- Œ≥: the discount factor (0 <= Œ≥ <= 1)\n",
        "- Q(s', a'): the Q-value for the next state 's' and the next action 'a'\n"
      ],
      "metadata": {
        "id": "XlABHP-UiIeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "In the following section, you are required to implement SARSA"
      ],
      "metadata": {
        "id": "YjXLNWPYt9XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: RL Sutton book, 6.4\n",
        "def SARSA(env_obj, num_episodes, episode_length=100, discount_factor=1.0, epsilon=0.1, seed=SEED, alpha=0.5, verbose=True):\n",
        "    \"\"\"\n",
        "    > For each episode, we run a loop for a maximum of `episode_length` steps. In each step, we take an\n",
        "    action according to the epsilon-greedy policy, observe the reward and next state, and update the\n",
        "    Q-value of the state-action pair\n",
        "\n",
        "    :param env_obj: The environment object that include a gym environment to run the algorithm on\n",
        "    :param num_episodes: the number of episodes to run the algorithm for\n",
        "    :param episode_length: The maximum length of an episode, defaults to 100 (optional)\n",
        "    :param discount_factor: the discount factor for the MDP (optional)\n",
        "    :param epsilon: the probability of choosing a random action (optional)\n",
        "    :param seed: The random seed to use for the environment     (optional)\n",
        "    :param alpha: the learning rate (optional)\n",
        "    :param verbose: If True, it will print the progress of the algorithm each specific frequency, defaults to True (optional)\n",
        "    :return:\n",
        "        * Q-function: a dictionary, such that the key is (state, action) and the value is the action-state-value\n",
        "        * policy: an epsilon greedy policy used during the training based on the computed Q-table\n",
        "        * stats: a dictionary, such that it has two keys rew & len, and the values are the total reward and length of the episode, respectively.\n",
        "    \"\"\"\n",
        "    env = env_obj.env\n",
        "    logging_freq = int(num_episodes*0.1)\n",
        "    fix_seed(env, seed)\n",
        "    # A nested dictionary that maps state -> (action -> action-value).\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # Store the reward and length of episodes during the running of the method\n",
        "    stats = defaultdict(list)\n",
        "\n",
        "    ######## TODO ########\n",
        "    # Note (Important):\n",
        "\n",
        "    # For each episode, make sure that you call:\n",
        "    # * stats[\"rew\"].append(cumulative_reward)\n",
        "    # * stats[\"len\"].append(episode_length)\n",
        "    # Such that cumulative_reward is the total reward in the episode\n",
        "    #  and episode_length stores the length of the episode\n",
        "    # * if verbose and (episode_idx+1)%logging_freq == 0:\n",
        "    #       env_obj.plot_Vfunction_from_Q(Q)\n",
        "\n",
        "    # Hint: you can use: policy = partial(epsilon_greedy_policy_fn, Q=Q, epsilon=epsilon, env=env)\n",
        "\n",
        "    # Hint: Q dictionary has the following form to access its values: Q[state][action]\n",
        "\n",
        "\n",
        "    ######################\n",
        "    return Q, policy, stats"
      ],
      "metadata": {
        "id": "DmG7Jwu_ivKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_obj = cliffwalking\n",
        "sarsa_Q, policy, stats = SARSA(env_obj, num_episodes=1000, episode_length=1000)\n",
        "env_obj.plot_Vfunction_from_Q(sarsa_Q)\n",
        "env_obj.plot_episode_stats(stats)"
      ],
      "metadata": {
        "id": "vxkVh0bTivbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the policy from the Q-function\n",
        "policy = lambda s: np.argmax(sarsa_Q[s])\n",
        "run_env_w_policy(env_obj.rendered_env, policy, steps=50)"
      ],
      "metadata": {
        "id": "rvh9K8FXA98P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "For CliffWalking environment, do the following:\n",
        "\n",
        "- Plot the state-value function using a random policy\n",
        "- Plot the state-value function using the trained policy by Q-Learning\n",
        "- Plot the state-value function using the trained policy by SARSA\n",
        "\n",
        "Then answer the questions stated in the following todo.\n",
        "\n",
        "Hint: You may try to estimate the policy using MC or TD0 and discuss the observed results, you may change the number of episodes for different state-value function predictions. (You may add extra cells if it is necessary for your analysis)"
      ],
      "metadata": {
        "id": "MwCSvday9jWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO ########\n",
        "# Plot the state-value fucntion using a random policy\n",
        "\n",
        "######################"
      ],
      "metadata": {
        "id": "1e2dXik09wf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO ########\n",
        "# Plot the state-value fucntion using the learnt policy from Q-Learning\n",
        "\n",
        "######################"
      ],
      "metadata": {
        "id": "yeBU8i2_-cJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO ########\n",
        "# Plot the state-value fucntion using the learnt policy from SARSA\n",
        "\n",
        "######################"
      ],
      "metadata": {
        "id": "y7ecl5i5-cii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "From your observation of plots above, answer the following question in the following cell:\n",
        "\n",
        "Q: Can you describe the optimal policies learnt by the RL algorithms? What do you observe from these plots?\n",
        "\n",
        "Q: What do you find differently when you use MC and TD for different policies?"
      ],
      "metadata": {
        "id": "pbkWYJhS9vXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "üõë <mark>TODO: Answer</mark>\n",
        "\n",
        "(No more than 100 words for each question)\n",
        "\n",
        "--------------\n"
      ],
      "metadata": {
        "id": "YCwCsprT-ZrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë <mark>TODO:</mark>\n",
        "\n",
        "Answer the following question in the following cell:\n",
        "\n",
        "Q: What is the difference between Q-Learning and SARSA? and when do you think it is good to use on-policy RL algorithm and when to use off-policy RL algorithm?\n",
        "\n"
      ],
      "metadata": {
        "id": "Dc2TL_EP6p5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "üõë <mark>TODO: Answer</mark>\n",
        "\n",
        "(No more than 100 words)\n",
        "\n",
        "\n",
        "\n",
        "--------------\n"
      ],
      "metadata": {
        "id": "jmWS5Bol9WHP"
      }
    }
  ]
}
